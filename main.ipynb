{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 1: Accept search term from user and\n",
    "# download last 100 tweets about it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import twitter\n",
    "\n",
    "api = twitter.Api(\n",
    "    consumer_key = \"ZrRO6Q1rjB8pjrfT6vS33em1Z\",\n",
    "    consumer_secret = \"r2rkhELbheAk4YzgKrZT780KyJnkUhBbMP3x6Hz1DV0qSHGHOS\",\n",
    "    access_token_key = \"559857675-V3d6JUsBoMSlXBEZnO37x0bCJHVogMhRTNrl1PAu\",\n",
    "    access_token_secret = \"6W3atnDdn3i4IrlddjiEu8XAdfAwUApWXVsqu7gzZUfL3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are we searching for today?apple\n"
     ]
    }
   ],
   "source": [
    "# Setup function which will accept a search term\n",
    "# and fetch tweets for that term.\n",
    "\n",
    "def createTestData(searchString):\n",
    "    try:\n",
    "        tweets_fetched = api.GetSearch(searchString, count=100)\n",
    "        # This returns a list of twitter.Status objects\n",
    "        # These have attributes for text, hashtags, etc\n",
    "        return [{\"text\": status.text, \"label\": None} for status in tweets_fetched]\n",
    "    except:\n",
    "        print \"Sorry, there was an error.\"\n",
    "        return None\n",
    "    \n",
    "search_string = raw_input(\"What are we searching for today?\")\n",
    "testData = createTestData(search_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 2: Classify each tweet as positive or negative\n",
    "\n",
    "# 2a: Download a corpus of tweets which can be used as training data\n",
    "# We'll use Niek Sander's Tweet Sentiment Corpus (5k+ labelled tweets)\n",
    "# Write a function to read csv, fetch tweet id and label, then download\n",
    "# the respective tweet from twitter, then write it back to another csv\n",
    "\n",
    "# Note: Since twitter has a limit of downloading only 180 tweets per \n",
    "# 15 mins, we need around 10 hours for this function to finish downloading\n",
    "# all the 5000 tweets\n",
    "\n",
    "def createTrainingCorpus(corpusFile, tweetDataFile):\n",
    "    import csv\n",
    "    corpus = []\n",
    "    with open(corpusFile, \"rb\") as csvFile:\n",
    "        lineHeader = csv.reader(csvFile, delimiter=',', quotechar='\\\"')\n",
    "        for row in lineHeader:\n",
    "            corpus.append({\"tweet_id\": row[2], \"label\": row[1], \"topic\": row[0]})\n",
    "    \n",
    "    # Twitter has a rate limit of 180 tweets/15 mins. So, we need to use \n",
    "    # a delay after each fetch.\n",
    "    \n",
    "    import time\n",
    "    rate_limit = 180\n",
    "    sleep_time = 900 / 180\n",
    "    \n",
    "    trainingData = []\n",
    "    for tweet in corpus:\n",
    "        try:\n",
    "            status = api.GetStatus(tweet['tweet_id'])\n",
    "            tweet['text'] = status.text\n",
    "            trainingData.append(tweet)\n",
    "            try:\n",
    "                with open(tweetDataFile, 'ab') as csvFile:\n",
    "                    linewriter = csv.writer(csvFile, delimiter=',', quotechar=\"\\\"\")\n",
    "                    linewriter.writerow([tweet['tweet_id'], tweet['text'], tweet['label'], tweet['topic']])\n",
    "            except:\n",
    "                continue\n",
    "            time.sleep(sleep_time)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return trainingData\n",
    "\n",
    "def createTrainingCorpusFromFile(tweetDataFile):\n",
    "    import csv\n",
    "    trainingData = []\n",
    "    with open(tweetDataFile, \"rb\") as csvFile:\n",
    "        lineHeader = csv.reader(csvFile, delimiter=',', quotechar='\\\"')\n",
    "        for row in lineHeader:\n",
    "            trainingData.append({\n",
    "                \"tweet_id\": row[0],\n",
    "                \"label\": row[2],\n",
    "                \"topic\": row[3],\n",
    "                \"text\": row[1],\n",
    "            })\n",
    "    return trainingData\n",
    "            \n",
    "\n",
    "corpusFile = \"/home/scarecrow/Downloads/sanders-twitter-0.2/corpus.csv\"\n",
    "tweetDataFile = \"modifiedCorpus.csv\"\n",
    "\n",
    "# Use this if running for the first time:\n",
    "\n",
    "trainingData = createTrainingCorpus(corpusFile, tweetDataFile)\n",
    "\n",
    "# Use this if tweets have been downloaded already and saved to a file\n",
    "# to save time on downloading tweets again: \n",
    "# trainingData = createTrainingCorpusFromFile(tweetDataFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#2b: A class to prepare all the tweets. Both test and training\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "class PreProcessTweets:\n",
    "    def __init__(self):\n",
    "        self._stopwords = set(stopwords.words('english') + list(punctuation) + ['AT_USER', 'URL', '\\'s'])\n",
    "        \n",
    "    def processTweets(self, list_of_tweets):\n",
    "        # List of tweets is a list of dicts\n",
    "        processedTweets = []\n",
    "        # List of tuples which is a list of words and its label\n",
    "        for tweet in list_of_tweets:\n",
    "            processedTweets.append((self._processTweet(tweet['text']), tweet['label']))\n",
    "        return processedTweets\n",
    "    \n",
    "    def _processTweet(self, tweet):\n",
    "        # 1. Convert to lowercase\n",
    "        tweet = tweet.lower()\n",
    "        # 2. Replace links with URL\n",
    "        tweet = re.sub(\"((www\\.[^\\s]+)|(https?://[^\\s]+))\", \"URL\", tweet)\n",
    "        # 3. Replace @<username> with AT_USER\n",
    "        tweet = re.sub(\"@[^\\s]+\", \"AT_USER\", tweet)\n",
    "        # 4. Replace #word with word\n",
    "        tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "        \n",
    "        tweet = word_tokenize(tweet)\n",
    "        \n",
    "        return [word for word in tweet if word not in self._stopwords]\n",
    "    \n",
    "tweetProcessor = PreProcessTweets()\n",
    "ppTrainingData = tweetProcessor.processTweets(trainingData)\n",
    "ppTestData = tweetProcessor.processTweets(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#2c: Extract features and train your classifier\n",
    "# Use 2 methods - Naive Baye's and SVM\n",
    "\n",
    "import nltk\n",
    "\n",
    "# i) Naive Baye's - We'll use NLTK's built in classifier\n",
    "\n",
    "# First, build vocabulary\n",
    "def buildVocabulary(ppTrainingData):\n",
    "    all_words = []\n",
    "    for [words, sentiment] in ppTrainingData:\n",
    "        if sentiment in ['positive', 'negative']:\n",
    "            all_words.extend(words)\n",
    "        # words are not deduped\n",
    "    wordlist = nltk.FreqDist(all_words)\n",
    "    # This creates a dict of words and its frequencies\n",
    "    word_features =wordlist.keys() \n",
    "    return word_features\n",
    "\n",
    "# NLTK has a apply_features function, which takes in a user defined function to extract features\n",
    "# from training data. We want to define our own extract features function to take each tweet in\n",
    "# the training data and represent it with the presence or absence of a word in the vocabulary.\n",
    "\n",
    "def extract_features(tweet):\n",
    "    tweet_words = set(tweet)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in tweet_words)\n",
    "    return features\n",
    "\n",
    "word_features = buildVocabulary(ppTrainingData)\n",
    "trainingFeatures = nltk.classify.apply_features(extract_features, [tweet for tweet in ppTrainingData if tweet[1] in ['positive', 'negative']])\n",
    "NBayesClassifier = nltk.NaiveBayesClassifier.train(trainingFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ii) SVM\n",
    "\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# We have to change the format of the data slightly as\n",
    "# SKLearn has a CountVectorizer object. It will take\n",
    "# documents and directly return a term-document matrix\n",
    "# with the frequencies of a word in the document. It \n",
    "# builds the vocabulary by itself. We will give the \n",
    "# trainingData and the labels separately to the SVM\n",
    "# classifier and not as tuples.\n",
    "\n",
    "svmTrainingData = [' '.join(tweet[0]) for tweet in ppTrainingData]\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "X = vectorizer.fit_transform(svmTrainingData).toarray()\n",
    "vocabulary = vectorizer.get_feature_names()\n",
    "\n",
    "# Now, we'll use sentiwordnet to add some weight to these features\n",
    "\n",
    "swn_weights = []\n",
    "\n",
    "for word in vocabulary:\n",
    "    try:\n",
    "        synset = list(swn.senti_synsets(word))\n",
    "        \n",
    "        # use the first synset only to compute the score, as this \n",
    "        # is the most common usage of the word.\n",
    "        \n",
    "        common_meaning = synset[0]\n",
    "        \n",
    "        # If posScore is greater, use that as weight, if neg_score is greater, \n",
    "        # use -neg_score as weight.\n",
    "        \n",
    "        if common_meaning.pos_score() > common_meaning.neg_score():\n",
    "            weight = common_meaning.pos_score()\n",
    "        elif common_meaning.pos_score() < common_meaning.neg_score():\n",
    "            weight = -1 * common_meaning.neg_score()\n",
    "        else:\n",
    "            weight = 0\n",
    "            \n",
    "    except Exception as e:\n",
    "        weight = 0\n",
    "    \n",
    "    swn_weights.append(weight)\n",
    "        \n",
    "# Let's now multiply each array in our original; matrix with these weights\n",
    "# Initialize a list\n",
    "\n",
    "swn_X = []\n",
    "for row in X:\n",
    "    swn_X.append(np.multiply(row, np.array(swn_weights)))\n",
    "\n",
    "# Convert list to np array\n",
    "swn_X = np.vstack(swn_X[:877])\n",
    "# Used hack to match shape of swn_X and y for the SVN.fit() function\n",
    "# TODO: Figure out the bug.\n",
    "\n",
    "# We have our documents ready. Let's get labels ready now\n",
    "# Let's map positive to 1 and negative to 2\n",
    "\n",
    "labels_to_array = {\"positive\": 1, \"negative\": 2}\n",
    "labels = [labels_to_array[tweet[1]] for tweet in ppTrainingData if tweet[1] in labels_to_array]\n",
    "y = np.array(labels)\n",
    "\n",
    "# Now, let's build the SVM classifier (a binary classifier)\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "SVMClassifier = SVC()\n",
    "SVMClassifier.fit(swn_X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2d: Run classifier on the 100 downloaded tweets, and classify them as positive or negative.\n",
    "\n",
    "# i) Naive Baye's:\n",
    "\n",
    "NBResultLabels = [NBayesClassifier.classify(extract_features(tweet[0])) for tweet in ppTestData]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ii) SVM:\n",
    "\n",
    "SVMResultLabels = []\n",
    "for tweet in ppTestData:\n",
    "    tweet_sentence = ' '.join(tweet[0])\n",
    "    svmFeatures = np.multiply(vectorizer.transform([tweet_sentence]).toarray(), np.array(swn_weights))\n",
    "    SVMResultLabels.append(SVMClassifier.predict(svmFeatures[0]))\n",
    "    \n",
    "    # predict() returns a numpy array, get the first element of the array\n",
    "    # There is only one element in the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB Result Negative Sentiment 62%\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Get majority vote and print the sentiment\n",
    "\n",
    "#i) Naive Baye's\n",
    "\n",
    "if NBResultLabels.count('positive') > NBResultLabels.count('negative'):\n",
    "    print \"NB Result Positive Sentiment \" + str(100 * NBResultLabels.count('positive') / len(NBResultLabels)) + \"%\"\n",
    "else:\n",
    "    print \"NB Result Negative Sentiment \" + str(100 * NBResultLabels.count('negative') / len(NBResultLabels)) + \"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Result Negative Sentiment 100%\n"
     ]
    }
   ],
   "source": [
    "# ii) SVM\n",
    "\n",
    "if SVMResultLabels.count(1) > SVMResultLabels.count(2):\n",
    "    print \"SVM Result Positive Sentiment \" + str(100 * SVMResultLabels.count(1) / len(SVMResultLabels)) + \"%\"\n",
    "else:\n",
    "    print \"SVM Result Negative Sentiment \" + str(100 * SVMResultLabels.count(2) / len(SVMResultLabels)) + \"%\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
